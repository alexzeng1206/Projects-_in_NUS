{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"0717_5_exercise_binary_food_classification_use_cnn.ipynb","version":"0.3.2","provenance":[{"file_id":"1aS0ofIm7d5lYyQ5I3ariImbo9mj47AgF","timestamp":1563281711394}],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"pXiR3libycbM","colab_type":"text"},"source":["# [Binary Classification : Food Recognition]\n","In this notebook, we will show how to use multi-layer CNN to build a binary food image classification project, to distinguish food images from nonfood images.\n","\n","Before running any code, we do the following two steps:\n","1. Reset the runtime by going to **Runtime -> Reset all runtimes** in the menu above. \n","2. Select GPU by going to **Runtime -> Change runtime type -> Hardware accelerator** in the menu above. \n"]},{"cell_type":"markdown","metadata":{"id":"I6ZP7vqrjHIn","colab_type":"text"},"source":["## [Importing packages]\n","\n","First, we will import some packages which will be used in our codes.\n","- `os` : for read files \n","- `numpy`: for matrix computation \n","- `matplotlib.pyplot`: for graph plotting and display\n","- `tensorflow`: the main deep learning toolbox\n","- `tf.keras.preprocessing.image.ImageDataGenerator`: for load data\n"]},{"cell_type":"code","metadata":{"id":"RviFoo5rob3_","colab_type":"code","colab":{}},"source":["from __future__ import absolute_import, division, print_function, unicode_literals\n","\n","import os\n","import numpy as np\n","import matplotlib.pyplot as plt\n","\n","import tensorflow as tf\n","from tensorflow.keras.preprocessing.image import ImageDataGenerator"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"OOTW9q2_g9gb"},"source":["## [Data Loading]\n","- Food-5K   \n","This is a dataset containing 2500 food and 2500 non-food images, for the task of food/non-food classification. The whole dataset is divided in three parts: training, validation and evaluation. The naming convention is as follows:\n","{ClassID}_{ImageID}.jpg\n","ClassID: 0 or 1; 0 means non-food and 1 means food.\n","ImageID: ID of the image within the class.    \n","More info can be seen from [Food/Non-food Image Classification and Food Categorization using Pre-Trained GoogLeNet Model](https://infoscience.epfl.ch/record/221610/files/madima2016_food_recognition.pdf)\n","\n","  The dataset is organized as follows:\n","- Food-5K\n","  - training   \n","    - food\n","    - non-food   \n"," - validation   \n","   - food\n","   - non-food\n"," - evaluation\n","   - food\n","   - non-food\n","          "]},{"cell_type":"code","metadata":{"colab_type":"code","id":"q5axRXNXg9gU","colab":{}},"source":["## get_file has some problem with google drive or dropbox link\n","#_URL =  'https://www.dropbox.com/s/zco960drgtjokht/Food-5K.zip'\n","#zip_dir = tf.keras.utils.get_file('Food_5K.zip', origin=_URL, extract=True)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"r6VZmJ2jfxHa","colab_type":"code","colab":{}},"source":["if not os.path.exists('/root/.keras/datasets/'):\n","    os.makedirs('/root/.keras/datasets/')"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"4_vt-prnQSMU","colab_type":"code","colab":{}},"source":["%cd /root/.keras/datasets/\n","!wget https://www.dropbox.com/s/zco960drgtjokht/Food-5K.zip\n","!unzip -uq Food-5K.zip"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"wzAyBQlFuk4f"},"source":["We'll now assign variables with the proper file path for the training and validation sets.\n"]},{"cell_type":"code","metadata":{"id":"oFjZ_S6Aso8c","colab_type":"code","colab":{}},"source":["base_dir = '/root/.keras/datasets/Food-5K'\n","train_dir = os.path.join(base_dir, 'training')\n","validation_dir = os.path.join(base_dir, 'validation')\n","\n","test_dir = os.path.join(base_dir, 'evaluation')"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"KYj2zNVPvS-j","colab_type":"code","colab":{}},"source":["train_food_dir = os.path.join(train_dir, 'food')   # directory with our training food pictures\n","train_nonfood_dir = os.path.join(train_dir,'non-food')   # directory with our training non-food pictures\n","validation_food_dir = os.path.join(validation_dir, 'food')  \n","validation_nonfood_dir = os.path.join(validation_dir,'non-food')\n","\n","test_food_dir =  os.path.join(test_dir, 'food')\n","test_nonfood_dir = os.path.join(test_dir,'non-food')\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Phauid9CqhBB","colab_type":"text"},"source":["### Data exploration\n"]},{"cell_type":"code","metadata":{"id":"42ewi1lWysRG","colab_type":"code","colab":{}},"source":["num_food_tr = len(os.listdir(train_food_dir))\n","num_nonfood_tr = len(os.listdir(train_nonfood_dir))\n","\n","num_food_val = len(os.listdir(validation_food_dir))\n","num_nonfood_val = len(os.listdir(validation_nonfood_dir))\n","\n","total_train = num_food_tr + num_nonfood_tr\n","total_val = num_food_val + num_nonfood_val\n","\n","\n","num_food_test = len(os.listdir(test_food_dir))\n","num_nonfood_test = len(os.listdir(test_nonfood_dir))\n","total_test = num_food_test + num_nonfood_test"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"_p-4Oqp68_IG","colab_type":"code","colab":{}},"source":["print('total training food images:', num_food_tr )\n","print('total training non-food images:', num_nonfood_tr)\n","\n","print('total validation food images:', num_food_val)\n","print('total validation non-food images:', num_nonfood_val)\n","print(\"--\")\n","print(\"Total training images:\", total_train)\n","print(\"Total validation images:\", total_val)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"WVx4e1qaZuRZ","colab_type":"text"},"source":["### Define several model parameters\n","Here, we will set up variables that will be used later while pre-processing our dataset and training our network."]},{"cell_type":"code","metadata":{"id":"OK0QXhwgaJHR","colab_type":"code","colab":{}},"source":["BATCH_SIZE = 30\n","IMG_SHAPE = 150"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"59u9N3qPXYEy","colab_type":"text"},"source":["### Data preparation\n","\n","Images must be formatted into appropriately pre-processed floating point tensors before being fed into the network. We use `tf.keras.preprocessing.image.ImageDataGenerator` to do data preparation.\n"," \n","- Read images from the disk\n","- Convert them into floating point tensors for usage in networks\n","- Rescale the tensors from values between 0 and 255 to values between 0 and 1."]},{"cell_type":"code","metadata":{"id":"ynvoWKq98Bt5","colab_type":"code","colab":{}},"source":["datagen = ImageDataGenerator(rescale=1./255)  "],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"j0jpjIkjYIqy","colab_type":"text"},"source":["`flow_from_directory` method can load images from the disk, and apply rescaling, and resize on images."]},{"cell_type":"code","metadata":{"id":"AYFDa25q00Lx","colab_type":"code","colab":{}},"source":["train_datagen = datagen.flow_from_directory(directory=train_dir, \n","                                                           shuffle=True, \n","                                                           target_size=(IMG_SHAPE,IMG_SHAPE), #(150,150) \n","                                                           batch_size=BATCH_SIZE, \n","                                                           class_mode='binary')\n","val_datagen = datagen.flow_from_directory(directory=validation_dir, \n","                                                           shuffle=False, \n","                                                           target_size=(IMG_SHAPE,IMG_SHAPE), #(150,150) \n","                                                           batch_size=BATCH_SIZE, \n","                                                           class_mode='binary')"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"gCnP6LKi476_","colab_type":"text"},"source":["Here, we show some image samples from the dataset."]},{"cell_type":"code","metadata":{"id":"GoNWilvM5BPv","colab_type":"code","colab":{}},"source":["# plots images with labels \n","def plotImg(ims, figsize=(20,20),titles=None):  \n","    f = plt.figure(figsize=figsize)\n","    for i in range(12):\n","        sp = f.add_subplot(4, 6, i+1)\n","        sp.axis('Off')\n","        if titles is not None:\n","            sp.set_title(titles[i], fontsize=15)\n","        plt.imshow(ims[i])\n","        \n","imgs, labels = next(train_datagen)\n","plotImg(imgs, titles=labels.astype(int))\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"wPFmZCa7cxpX","colab_type":"text"},"source":["## [Construct the Model]\n","\n","We build a model with 3 convolution layers with max-pooling after each conv layer and 2 fully connected layer.\n","activation function -- 'relu' "]},{"cell_type":"code","metadata":{"id":"slHQp8fpdjc7","colab_type":"code","colab":{}},"source":["model = tf.keras.models.Sequential([\n","    tf.keras.layers.Conv2D(32,(3,3),activation='relu',input_shape=(150,150,3)),\n","    tf.keras.layers.MaxPooling2D(2, 2),\n","    \n","    tf.keras.layers.Conv2D(64, (3,3), activation='relu'),\n","    tf.keras.layers.MaxPooling2D(2,2),\n","    \n","    tf.keras.layers.Conv2D(128, (3,3), activation='relu'),\n","    tf.keras.layers.MaxPooling2D(2,2),\n","    \n","    tf.keras.layers.Flatten(),\n","    tf.keras.layers.Dense(512, activation='relu'),\n","    tf.keras.layers.Dense(2, activation='softmax')\n","])"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"EShkJAG-fBfu","colab_type":"text"},"source":["Here, we set up the optimizer and loss function.\n","We use `adam` optimizer,  `sparse_categorical_crossentropy` as the loss function, `accuracy` as the performance measure."]},{"cell_type":"code","metadata":{"id":"xyB7lT1le10d","colab_type":"code","colab":{}},"source":["model.compile(optimizer='adam',\n","             loss = 'sparse_categorical_crossentropy',\n","             metrics=['accuracy'])"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"mtNAmthMfeZm","colab_type":"text"},"source":["We can see the model architecture by using the `summary` function."]},{"cell_type":"code","metadata":{"id":"4xMdxq4Qfes2","colab_type":"code","colab":{}},"source":["model.summary()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"uA2Rdh40f4LH","colab_type":"text"},"source":["## [Training Model]"]},{"cell_type":"code","metadata":{"id":"452rJrtFgGmn","colab_type":"code","colab":{}},"source":["EPOCHS = 20\n","history = model.fit_generator(\n","    train_datagen,\n","    steps_per_epoch=int(np.ceil(total_train / float(BATCH_SIZE))),\n","    epochs=EPOCHS,\n","    validation_data=val_datagen,\n","    validation_steps=int(np.ceil(total_val / float(BATCH_SIZE)))\n",")"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"qaGlwHljkt-l","colab_type":"text"},"source":["### Visualize the training process   \n","Here, we visualize the training process to have a better understanding of our model.\n","To see the performance of our model, and get some insights on how to tune our model for better performance."]},{"cell_type":"code","metadata":{"id":"OPe9WHonktXw","colab_type":"code","colab":{}},"source":["acc = history.history['accuracy']\n","val_acc = history.history['val_accuracy']\n","\n","loss = history.history['loss']\n","val_loss = history.history['val_loss']\n","\n","epochs_range = range(EPOCHS)\n","\n","plt.figure(figsize=(8,8))\n","plt.subplot(1, 2, 1)\n","plt.plot(epochs_range, acc, label='Training Accuracy')\n","plt.plot(epochs_range, val_acc, label='Validation Accuracy')\n","plt.legend(loc='lower right')\n","plt.title('Training and Validation Accuracy')\n","\n","plt.subplot(1, 2, 2)\n","plt.plot(epochs_range, loss, label='Training Loss')\n","plt.plot(epochs_range, val_loss, label='Validation Loss')\n","plt.legend(loc='upper right')\n","plt.title('Training and Validation Loss')\n","plt.savefig('./foo.png')\n","plt.show()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"x7YFu0eSlqtQ","colab_type":"text"},"source":["## [Evaluating Model]\n","It is always a good practice to evaludate our models on held out samples.\n","We use the training data to learn the neural networks parameters, such as the weights and bias.\n","We use the validation data to verify whether our model is overfitting on the training data, to balance our model to get good performance on both training and validation data.\n","And finally, the test data is totally un-used in our development of our model. And its performance is generally fair enough to compare different models.\n","\n"]},{"cell_type":"code","metadata":{"id":"gu3miHKxnI59","colab_type":"code","colab":{}},"source":["test_datagen = datagen.flow_from_directory(directory=test_dir, \n","                                                           shuffle=False, \n","                                                           target_size=(IMG_SHAPE,IMG_SHAPE), #(150,150) \n","                                                           batch_size=BATCH_SIZE, \n","                                                           class_mode='binary')\n","\n","score = model.evaluate_generator(generator=test_datagen,\n","                                 steps = int(np.ceil(total_test / float(BATCH_SIZE))))\n","print(\"Test Loss: \", score[0], \"Test Accuracy: \", score[1])"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"LLBzAMhGnMfM","colab_type":"text"},"source":["## [Save and Load Model]"]},{"cell_type":"code","metadata":{"id":"vhV-feesnVW1","colab_type":"code","colab":{}},"source":["from keras.models import load_model\n","\n","model.save('my_model.h5') \n","\n","# del model    # deletes the existing model\n","\n","# model = tf.keras.models.load_model('my_model.h5')"],"execution_count":0,"outputs":[]}]}